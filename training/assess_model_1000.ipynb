{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Accuracy\n",
    "\n",
    "#### Set Up Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Root directory of MRCNN\n",
    "MASK_RCNN_DIR = os.path.abspath(\"../../Mask_RCNN/\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(MASK_RCNN_DIR)  # To find local version of the library\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log\n",
    "\n",
    "import oyster\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = oyster.OysterConfig()\n",
    "OYSTER_DIR = \"../data/1kx1k_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.02\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                14\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'mrcnn_mask_loss': 2.0, 'mrcnn_bbox_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'rpn_class_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           oyster\n",
      "NUM_CLASSES                    2\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              2\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                103\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Override the training configurations with a few\n",
    "# changes for inferencing.\n",
    "class InferenceConfig(config.__class__):\n",
    "    # Run detection on one image at a time\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device to load the neural network on.\n",
    "# Useful if you're training a model on the same \n",
    "# machine, in which case use CPU and leave the\n",
    "# GPU for training.\n",
    "DEVICE = \"/gpu:2\"  # /cpu:0 or /gpu:0\n",
    "\n",
    "# Inspect the model in training or inference modes\n",
    "# values: 'inference' or 'training'\n",
    "# TODO: code for 'training' test mode not ready yet\n",
    "TEST_MODE = \"inference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=16):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Adjust the size attribute to control how big to render images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: 163\n",
      "Classes: ['BG', 'oyster']\n"
     ]
    }
   ],
   "source": [
    "# Load validation dataset\n",
    "dataset_test = oyster.OysterDataset()\n",
    "dataset_test.load_oyster(OYSTER_DIR, \"test\")\n",
    "\n",
    "# Must call before using the dataset\n",
    "dataset_test.prepare()\n",
    "\n",
    "print(\"Images: {}\\nClasses: {}\".format(len(dataset_test.image_ids), dataset_test.class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "Loading weights  ../logs/oyster20190122T0034/mask_rcnn_oyster_0083.h5\n",
      "Re-starting from epoch 83\n"
     ]
    }
   ],
   "source": [
    "# Create model in inference mode\n",
    "with tf.device(DEVICE):\n",
    "    model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR,\n",
    "                              config=config)\n",
    "    \n",
    "# Set path to oyster weights file\n",
    "weights_path = '../data/models/1kx1k_model.h5'\n",
    "\n",
    "# Or, load the last model you trained\n",
    "#weights_path = model.find_last()\n",
    "\n",
    "# Load weights\n",
    "print(\"Loading weights \", weights_path)\n",
    "model.load_weights(weights_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overlaps_masks(masks1, masks2):\n",
    "    \"\"\"Computes IoU overlaps between two sets of masks.\n",
    "    masks1, masks2: [Height, Width, instances]\n",
    "    \"\"\"\n",
    "        \n",
    "    # If either set of masks is empty return empty result\n",
    "    if masks2.shape[-1] == 0:\n",
    "        overlaps = 0\n",
    "        intersections = 0\n",
    "        area2 = 0\n",
    "        masks1 = np.reshape(masks1 > .5, (-1, masks1.shape[-1])).astype(np.float32)\n",
    "        area1 = np.sum(masks1, axis=0)\n",
    "        union = area2\n",
    "        return(overlaps, intersections, union, area1, area2)\n",
    "    if masks1.shape[-1] == 0:\n",
    "        return(-1)\n",
    "    \n",
    "    # flatten masks and compute their areas\n",
    "    masks1 = np.reshape(masks1 > .5, (-1, masks1.shape[-1])).astype(np.float32)\n",
    "    masks2 = np.reshape(masks2 > .5, (-1, masks2.shape[-1])).astype(np.float32)\n",
    "    \n",
    "    # some of the masks overlap which throws off the recall values so we just take the max of all of them\n",
    "    masks2 = np.amax(masks2, axis=1)\n",
    "    \n",
    "    area1 = np.sum(masks1, axis=0)\n",
    "    area2 = np.sum(masks2, axis=0)\n",
    "\n",
    "    # intersections and union\n",
    "    intersections = np.dot(masks1.T, masks2)\n",
    "    union = area1[:, None] + area2[None] - intersections\n",
    "    overlaps = intersections / union\n",
    "\n",
    "    return(overlaps, intersections, union, area1, area2)\n",
    "\n",
    "def calculate_metrics(dataset, prediction_threshold=0.5):\n",
    "    total_gt = 0  # total ground truth reefs\n",
    "    total_predicted = 0 # total predicted reefs\n",
    "    total_tp = 0 # total true positives\n",
    "    total_fp = 0 # total false positives\n",
    "    total_px = 0 # all pixels\n",
    "    total_fn = 0\n",
    "    \n",
    "    count =0\n",
    "\n",
    "    for image_id in dataset.image_ids:\n",
    "        if count > 50:\n",
    "            break\n",
    "        # Load image and ground truth data\n",
    "        image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "            modellib.load_image_gt(dataset, config,\n",
    "                                   image_id, use_mini_mask=False)\n",
    "        molded_images = np.expand_dims(modellib.mold_image(image, config), 0)\n",
    "        # Run object detection\n",
    "        results = model.detect([image], verbose=0)\n",
    "        r = results[0]\n",
    "        \n",
    "        # filter out masks that don't meet a prediction threshold\n",
    "        r['masks'] = np.array(r['masks'][:,:, r['scores'] > prediction_threshold])\n",
    "\n",
    "        overlap, intersection, union, gt_area, mask_area = compute_overlaps_masks(gt_mask, r['masks'])\n",
    "            \n",
    "        total_tp += np.sum(intersection)\n",
    "        total_predicted += np.sum(mask_area)\n",
    "        total_gt += np.sum(gt_mask)\n",
    "        \n",
    "        #print(np.sum(gt_mask), np.sum(intersection))\n",
    "\n",
    "        total_fp += np.sum(mask_area) - np.sum(intersection) \n",
    "\n",
    "        total_fn += np.sum(gt_mask) - np.sum(intersection) \n",
    "\n",
    "        length, width, instances = gt_mask.shape\n",
    "        total_px += length * width\n",
    "        count +=1\n",
    "        \n",
    "    total_tn = total_px - total_predicted\n",
    "    return(total_tp, total_fp, total_fn, total_tn, total_gt, total_predicted, total_px)\n",
    "\n",
    "def calc_prec_recall_acc(total_tp, total_fp, total_fn, total_tn):\n",
    "    precision = total_tp / (total_tp + total_fp)\n",
    "    recall = total_tp / (total_tp + total_fn)\n",
    "    acc = (total_tp + total_tn) / total_px\n",
    "    return(precision, recall, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tp, total_fp, total_fn, total_tn, total_gt, total_predicted, total_px = calculate_metrics(dataset_test, prediction_threshold=0.95)\n",
    "precision, recall, acc = calc_prec_recall_acc(total_tp, total_fp, total_fn, total_tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5905753.0, 10426553.0, 675161.0, 37145070.0, 6580914, 16332306.0, 53477376)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tp, total_fp, total_fn, total_tn, total_gt, total_predicted, total_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.36159945815367406, 0.8974061961605941, 0.8050287097108131)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tp, total_fp, total_fn, total_tn, total_gt, total_predicted, total_px = calculate_metrics(dataset_test, prediction_threshold=0.95)\n",
    "precision, recall, acc = calc_prec_recall_acc(total_tp, total_fp, total_fn, total_tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15378072.0,\n",
       " 31483038.0,\n",
       " 3587594.0,\n",
       " 124056778.0,\n",
       " 18965666,\n",
       " 46861110.0,\n",
       " 170917888)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tp, total_fp, total_fn, total_tn, total_gt, total_predicted, total_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.32816277719413817, 0.8108374364496348, 0.8158002163003559)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(312666.0, 686774.0, 147460.0, 5292016.0, 460126, 999440.0, 6291456)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tp, total_fp, total_fn, total_tn, total_gt, total_predicted, total_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.991, 0.992, 0.993, 0.994, 0.995, 0.996, 0.997, 0.998, 0.999,\n",
       "       1.   ])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0.991, 1.0, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9999  , 0.999905, 0.99991 , 0.999915, 0.99992 , 0.999925,\n",
       "       0.99993 , 0.999935, 0.99994 , 0.999945, 0.99995 , 0.999955,\n",
       "       0.99996 , 0.999965, 0.99997 , 0.999975, 0.99998 , 0.999985,\n",
       "       0.99999 , 0.999995])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0.9999, 1.0, 0.000005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pra_list_extd = []\n",
    "for confidence in np.arange(0.9999, 1.0, 0.000005):\n",
    "    total_tp, total_fp, total_fn, total_tn, total_gt, total_predicted, total_px = calculate_metrics(dataset_test, prediction_threshold=confidence)\n",
    "    precision, recall, acc = calc_prec_recall_acc(total_tp, total_fp, total_fn, total_tn)\n",
    "    pra_list_extd.append([precision, recall, acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.28940885238435726, 0.8190219060029456, 0.7916780172609815],\n",
       " [0.29267387858171723, 0.8185603450750077, 0.7950641033696193],\n",
       " [0.29564480764747575, 0.8181486045679188, 0.798077246722053],\n",
       " [0.29880252555368514, 0.8176899317803201, 0.8012183320288565],\n",
       " [0.3031617820740539, 0.8175821680439185, 0.8053203657561657],\n",
       " [0.30580354746206856, 0.8174388440795921, 0.807767531451057],\n",
       " [0.3095044951248104, 0.8105397988446356, 0.8126732321346507],\n",
       " [0.31299404713003715, 0.8101253507059788, 0.8157921024397308],\n",
       " [0.3203835697038772, 0.8091215414293804, 0.8221970352472043],\n",
       " [0.32437122703779775, 0.7940970743679892, 0.8286551681219363],\n",
       " [0.3359881931930654, 0.790897736907383, 0.8380787980322745],\n",
       " [0.34849200547324133, 0.7815935928050648, 0.848630101073022],\n",
       " [0.36601936119546696, 0.777308495085739, 0.8605248133341471],\n",
       " [0.38532541175684265, 0.7575963957452638, 0.8748049642525467],\n",
       " [0.40755381644743477, 0.7549423752824286, 0.8863133823170382],\n",
       " [0.4417708103260365, 0.7166696420529118, 0.9061863282147575],\n",
       " [0.5092129142538765, 0.6796942253619517, 0.9321360868566176],\n",
       " [0.6068029443522888, 0.6087445308098689, 0.9591370937870998],\n",
       " [0.6725643931167583, 0.4930481559494698, 0.975133596682081],\n",
       " [0.7493124423187713, 0.2636561654485634, 0.9908622479906269]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pra_list_extd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:68: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "pra_list = []\n",
    "for confidence in np.arange(0.0, 1.01, 0.01):\n",
    "    total_tp, total_fp, total_fn, total_tn, total_gt, total_predicted, total_px = calculate_metrics(dataset_test, prediction_threshold=confidence)\n",
    "    precision, recall, acc = calc_prec_recall_acc(total_tp, total_fp, total_fn, total_tn)\n",
    "    pra_list.append([precision, recall, acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.991, 0.992, 0.993, 0.994, 0.995, 0.996, 0.997, 0.998, 0.999,\n",
       "       1.   ])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0.991, 1.0, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7202868710>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "pra_array = np.array(pra_list_extd)\n",
    "ax.set_xlabel(\"Recall\")\n",
    "ax.set_ylabel(\"Precision\")\n",
    "ax.set_xlim(0.28,0.82)\n",
    "ax.set_ylim(0.2,.8)\n",
    "ax.plot([i[1] for i in pra_array], [i[0] for i in pra_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP:  1.0\n"
     ]
    }
   ],
   "source": [
    "#image_ids = np.random.choice(dataset_test.image_ids, 53)\n",
    "\n",
    "APs = []\n",
    "AP_ranges = []\n",
    "precisions_list = []\n",
    "recalls_list = []\n",
    "\n",
    "total_gt = 0  # total ground truth reefs\n",
    "total_predicted = 0 # total predicted reefs\n",
    "total_tp = 0 # total true positives\n",
    "\n",
    "for image_id in dataset_test.image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_test, config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    precisions_list.append(precisions)\n",
    "    recalls_list.append(recalls)\n",
    "    \"\"\"\n",
    "    AP_range = utils.compute_ap_range(gt_bbox, gt_class_id, gt_mask,\n",
    "                     r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'],\n",
    "                     iou_thresholds=None, verbose=0)\n",
    "    AP_ranges.append(AP_range)\n",
    "    \"\"\"\n",
    "    gt_match, pred_match, overlaps = utils.compute_matches(gt_bbox, gt_class_id, gt_mask,\n",
    "                    r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'],\n",
    "                    iou_threshold=0.5, score_threshold=0.5)\n",
    "    \n",
    "    total_gt += gt_match.shape[0]  # total ground truth reefs\n",
    "    total_predicted += pred_match.shape[0] # total predicted reefs\n",
    "    total_tp += (pred_match > -1).sum() # total true positives\n",
    "    \n",
    "    break\n",
    "\n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_fp = total_predicted - total_tp\n",
    "total_fn = total_gt - total_tp\n",
    "\n",
    "# Precision is defined as the number of true positives over the number of true positives plus the number of false positives.\n",
    "\n",
    "precision = total_tp / (total_tp + total_fp)\n",
    "\n",
    "# Recall is defined as the number of true positives over the number of true positives plus the number of false negatives.\n",
    "recall = total_tp / (total_tp + total_fn )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
